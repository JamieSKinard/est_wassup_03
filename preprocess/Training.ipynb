{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kode-git/FER-Visual-Transformers/blob/main/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TedQm15ebAc4"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTCi9uLibNun"
      },
      "source": [
        "This notebook is used for trains transformers and deep neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z270ZRjMbXs_"
      },
      "source": [
        "## Install Dependencies and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX18-RD2bNE2",
        "outputId": "6ba859b1-cbb0-4f21-b39a-6d4d6fedc0c9"
      },
      "outputs": [],
      "source": [
        "!pip install timm\n",
        "!pip install fvcore\n",
        "!git clone https://github.com/davda54/sam.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWeinYu7a7O0"
      },
      "outputs": [],
      "source": [
        "# classic libraries for collections.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# utility library.\n",
        "import random, time, copy\n",
        "\n",
        "# plot libraries.\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "# libraries for image processing.\n",
        "import os, cv2, glob, imageio, sys\n",
        "from PIL import Image\n",
        "# warning library for service warnings.\n",
        "import warnings\n",
        "\n",
        "# machine learning libraries .\n",
        "import timm, torch, torchvision\n",
        "from torchsummary import summary\n",
        "\n",
        "# image dataset loading and transformations.\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "# utility functions for specific uses.\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "\n",
        "# optimizer libraries.\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.optim as optim\n",
        "from sam.sam import SAM\n",
        "\n",
        "# library for basic building blocks.\n",
        "import torch.nn as nn\n",
        "\n",
        "# library for saving and loading checkpoints.\n",
        "import pickle\n",
        "\n",
        "# libraries for metrics and evaluation phase.\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# libraries for flop analysis.\n",
        "from fvcore.nn import FlopCountAnalysis, flop_count_table, flop_count_str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnRVVSa1d6Ze",
        "outputId": "38670851-573b-42b5-b58e-c5ef87d600ed"
      },
      "outputs": [],
      "source": [
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCQDvI9IddjL"
      },
      "source": [
        "## GPU Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaNwNAVKdg0V"
      },
      "source": [
        "Transformers are trained using Google Colab Pro GPU: NVIDIA P100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO0H9JgcdgCk",
        "outputId": "4ac65945-33e9-41f0-fd4a-6ad54ae037cb"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP1OgXp2d1IO",
        "outputId": "d589f290-c10c-4bb6-e184-999dbac55566"
      },
      "outputs": [],
      "source": [
        "# Detect if we have a GPU available.\n",
        "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k_z_eI2ccIi"
      },
      "source": [
        "## Common utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpE6-fohcUWF"
      },
      "outputs": [],
      "source": [
        "def mkdir_model(base_dir, name_model, counter):\n",
        "  \"\"\"\n",
        "  Making a directory for the model dump.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    d = \"{}/{}\".format(base_dir,name_model)\n",
        "    os.mkdir(d)\n",
        "  except FileExistsError:\n",
        "    counter += 1\n",
        "    mkdir_model(base_dir, str(name_model) + \"_\" + str(counter), counter)\n",
        "\n",
        "def save_history(history, filename):\n",
        "  \"\"\"\n",
        "  Save the history in the file.\n",
        "  \"\"\"\n",
        "  if os.path.isfile(filename):\n",
        "    os.remove(filename)\n",
        "  file_handler = open(filename + \".pkl\", \"wb\")\n",
        "  pickle.dump(history, file_handler)\n",
        "  file_handler.close()\n",
        "\n",
        "\n",
        "def load_history(filename):\n",
        "  \"\"\"\n",
        "  Load the history from the file.\n",
        "  \"\"\"\n",
        "  file_handler = open(filename + \".pkl\", \"rb\")\n",
        "  output = pickle.load(file_handler)\n",
        "  file_handler.close()\n",
        "  return output # 가중치 파일 인식?\n",
        "\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer,lr_scheduler, num_epochs=25, is_inception=False, \n",
        "                is_loaded = False, load_state_ws=None, history_file_acc=\"history_accuracy\",\n",
        "                history_file_loss=\"history_loss\", n_partial=0, model_folder=\"\", best_acc=0.0 ):\n",
        "    \"\"\"\n",
        "    PyTorch training model with loading support and dump management.\n",
        "    Trains a model in a series of epochs and return the best configuration.\n",
        "    Best configuration is given by the best validation accuracy around epochs.\n",
        "    Training metrics are saved in well formated files.\n",
        "    \"\"\"\n",
        "    \n",
        "    history = {'val_2' : [], 'train_2' : []}\n",
        "    loss_history = {'val_2' : [], 'train_2' : []}\n",
        "\n",
        "    if is_loaded and load_state_ws != None:\n",
        "      # load the model.\n",
        "      state_dict = torch.load(load_state_ws)\n",
        "      model.load_state_dict(state_dict)\n",
        "      model.eval()\n",
        "      print('Model loaded correctly')\n",
        "\n",
        "    print('Starting Training')\n",
        "    print('-' * 12)\n",
        "\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = best_acc\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_since = time.time()\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "        print('-' * 12)\n",
        "        # Each epoch has a training and validation phase.\n",
        "        for phase in ['train_2', 'val_2']:\n",
        "            total = len(dataloaders[phase])\n",
        "            current = 0\n",
        "            if phase == 'train_2':\n",
        "                model.train()  # Set model to training mode.\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode.\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            dl = dataloaders[phase]\n",
        "            totalIm=0\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dl:\n",
        "                totalIm+=len(inputs)\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients.\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward.\n",
        "                # track history if only in train.\n",
        "                with torch.set_grad_enabled(phase == 'train_2'):\n",
        "                    # Get model outputs and calculate loss.\n",
        "                      outputs = model(inputs)\n",
        "                      loss = criterion(outputs, labels)\n",
        "\n",
        "                      _, preds = torch.max(outputs, 1)\n",
        "                      def closure():\n",
        "                          outputs = model(inputs)\n",
        "                          _, preds = torch.max(outputs, 1)\n",
        "                          loss = criterion(outputs, labels)\n",
        "                          loss.backward()\n",
        "                          return loss\n",
        "\n",
        "                    # backward + optimize only if in training phase.\n",
        "                      if phase == 'train_2':\n",
        "                        loss.backward()\n",
        "                        if type(optimizer) != SAM:\n",
        "                          optimizer.step()\n",
        "                        else:\n",
        "                          optimizer.step(closure)\n",
        "\n",
        "                        \n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                epoch_loss = running_loss / totalIm\n",
        "                epoch_acc = running_corrects.double() / totalIm\n",
        "                # status update.\n",
        "                current += 1\n",
        "                sys.stdout.write(\"\\r\" + f\"{epoch + 1}/{num_epochs} - {phase} step : \" + str(current * batch_size) + \"/\" +  str(total * batch_size) + \" - \" + \n",
        "                                 \"{}_accuracy : \".format(phase) + \"{:4f}\".format(epoch_acc) + \" - {}_loss : \".format(phase) + \"{:4f}\".format(epoch_loss))\n",
        "                sys.stdout.flush()\n",
        "            epoch_loss = running_loss / totalIm\n",
        "            epoch_acc = running_corrects.double() / totalIm\n",
        "            print() # avoid result cleaning .\n",
        "            if phase == 'train_2':\n",
        "              history['train_2'].append(epoch_acc)\n",
        "              loss_history['train_2'].append(epoch_loss)\n",
        "\n",
        "            # deep copy the model only in case the accusary is better in evaluation (local optima).\n",
        "            local_optima = False\n",
        "            if phase == 'val_test' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                local_optima = True\n",
        "            if phase == 'val_2':\n",
        "                history['val_2'].append(epoch_acc)\n",
        "                loss_history['val_2'].append(epoch_loss)\n",
        "\n",
        "        # Increases the internal counter.\n",
        "        if lr_scheduler:            \n",
        "            lr_scheduler.step()            \n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        interval_epoch = time.time() - epoch_since \n",
        "        print('\\nEpoch {} complete in. {:.0f}m {:.0f}s {} and with a learning rate of {}'.format(epoch + 1, interval_epoch // 60, interval_epoch % 60, \"with best local accuracy\" if local_optima else \"\",lr))\n",
        "        save_history(loss_history, model_folder + os.path.basename(model_folder) + \"_\" + history_file_loss)\n",
        "        \n",
        "        torch.save(model.state_dict(), model_folder + \"epoch_{}_{}\".format(epoch + 1, os.path.basename(model_folder[:len(model_folder) - 1])))\n",
        "        print(\"-\" * 12)\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val accuracy: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights.\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history['train_2'], history['val_2'], best_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DalOvLheWX1"
      },
      "source": [
        "## Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.classes = []  # 클래스 레이블을 저장하는 리스트 추가\n",
        "        self.load_images()\n",
        "\n",
        "    def load_images(self):\n",
        "        problematic_images = []\n",
        "        for i, class_name in enumerate(os.listdir(self.root_dir)):\n",
        "            class_dir = os.path.join(self.root_dir, class_name)\n",
        "            if not os.path.isdir(class_dir):\n",
        "                continue\n",
        "            for filename in os.listdir(class_dir):\n",
        "                if filename.endswith('.jpg'):\n",
        "                    img_path = os.path.join(class_dir, filename)\n",
        "                    try:\n",
        "                        with Image.open(img_path) as img:\n",
        "                            if self.transform:\n",
        "                                img = self.transform(img)\n",
        "                            self.images.append(img_path)\n",
        "                            self.labels.append(i)  # 클래스 레이블을 숫자로 저장\n",
        "                            self.classes.append(class_name)  # 클래스 이름 저장\n",
        "                    except Exception as e:\n",
        "                        print(f\"Skipping problematic image: {img_path}\")\n",
        "                        problematic_images.append(img_path)\n",
        "\n",
        "        # 문제가 있는 이미지를 데이터셋에서 제거\n",
        "        for img_path in problematic_images:\n",
        "            try:\n",
        "                idx = self.images.index(img_path)\n",
        "                del self.images[idx]\n",
        "                del self.labels[idx]\n",
        "                del self.classes[idx]\n",
        "            except ValueError:\n",
        "                print(f\"Skipping problematic image: {img_path} not found in the dataset\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        try:\n",
        "            with Image.open(img_path) as img:\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                return img, label\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping problematic image: {img_path}\")\n",
        "            \n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grsVit1sdUbT",
        "outputId": "29a29244-26d1-46a8-c382-5494629bf1f4"
      },
      "outputs": [],
      "source": [
        "# input and batch size specification.\n",
        "input_size = (224,224)\n",
        "batch_size = 30\n",
        "\n",
        "# dataset directory.\n",
        "data_dir= \"../../../../data/image/\"\n",
        "\n",
        "# removing possible .ipybn_checkpoints.\n",
        "for fd in glob.glob(\"../../../../data/image/*\"):\n",
        "  for cl in glob.glob(fd + \"/.*\"):\n",
        "    os.rmdir(cl)\n",
        "\n",
        "# loading training and validation set.\n",
        "data_transforms = {\n",
        "    'train_2': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "    'val_2': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "\n",
        "# 모델 수정 후 미세 조정\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create training and validation datasets.\n",
        "image_datasets = {x: CustomDataset(os.path.join(data_dir, x), transform=data_transforms[x]) for x in ['train_2', 'val_2']}\n",
        "\n",
        "# Create training and validation dataloaders.\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=7,pin_memory=True) for x in ['train_2', 'val_2']}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bw-eR3cern8"
      },
      "outputs": [],
      "source": [
        "# specify the total number of classes.\n",
        "NUM_CLASSES = 7\n",
        "model_name = 'vit_base_patch16_224'\n",
        "# model_name = 'resnet18'\n",
        "# loading pretrained model.\n",
        "model = timm.create_model(model_name, pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgCr7dMue_6n",
        "outputId": "3cb7ea0d-ca2c-49fb-bd19-7913098ac2a3"
      },
      "outputs": [],
      "source": [
        "# flops analysis.\n",
        "inputs = (torch.randn((1, 3, 224, 224)))\n",
        "model.eval() \n",
        "print('-'*40)\n",
        "\n",
        "# flop data display.\n",
        "flop = FlopCountAnalysis(model, inputs)\n",
        "print(flop_count_table(flop, max_depth=4))\n",
        "print(flop_count_str(flop))\n",
        "print(\"Tot. flops:\", flop.total())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTVMuJLafWtW",
        "outputId": "f2d33d29-8b10-49e3-ec13-9fa050dbe052"
      },
      "outputs": [],
      "source": [
        "# adapting head for 8 classes classify (fine-tuning).\n",
        "\n",
        "if model_name == 'resnet18':\n",
        "  model.fc = nn.Linear(512, NUM_CLASSES)\n",
        "else: \n",
        "  model.head = nn.Linear(768, NUM_CLASSES)\n",
        "  \n",
        "# display modified model.\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqQHhSRjgwKv",
        "outputId": "f29a4b7c-7a86-4f82-dc2e-082819cf7c1f"
      },
      "outputs": [],
      "source": [
        "optimizer_set = input('Digit 0 for SGD or other values for SAM: ')\n",
        "if optimizer_set == str(0):\n",
        "  optimizer_set = \"Adam\"\n",
        "else:\n",
        "  optimizer_set = \"AdamW\"\n",
        "print('Chosen {} for the model training.'.format(optimizer_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RB46k_5fqKF",
        "outputId": "3c8cb790-b377-408c-cf97-0f5aeb188392"
      },
      "outputs": [],
      "source": [
        "# Detect if we have a GPU available.\n",
        "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# Send the model to GPU\n",
        "model = model.to(device)\n",
        "feature_extract=True\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are\n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model.parameters()\n",
        "print(\"Params to learn:\")\n",
        "\n",
        "for name,param in model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "          print(\"\\t\",name)\n",
        "\n",
        "print('-'*40)\n",
        "lr_in = 0.001\n",
        "momentum_in = 0.9\n",
        "if optimizer_set == \"Adam\":\n",
        "  # stochasic gradient descent.\n",
        "  optimizer_ft = optim.Adam(params_to_update, lr=lr_in)\n",
        "else:\n",
        "  # shapeness-aware minimizer.\n",
        "  optimizer_ft = optim.Adam(model.parameters(), lr=lr_in)\n",
        "\n",
        "print(optimizer_ft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RJIMBC4joKT"
      },
      "source": [
        "## Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSQCVvBqh3OG",
        "outputId": "bea75a20-f23a-45b1-82b7-ada4dc560ee8"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = input('Digits the initial number of epochs, invalid values are equals to 10 epochs: ')\n",
        "try:\n",
        "  int(num_epochs)\n",
        "except ValueError:\n",
        "  print('Default number of 10 epochs selected.')\n",
        "  num_epochs = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1S7T5RYjR-h"
      },
      "outputs": [],
      "source": [
        "# model general info.\n",
        "name_model = \"vfer_small_5\"\n",
        "base_dir = \"../Models/\"\n",
        "\n",
        "# model files for saving history and model data.\n",
        "model_folder = base_dir + name_model + \"/\"\n",
        "model_file = model_folder + name_model + \".pth\"\n",
        "train_history = model_folder + name_model + \"_\" + \"history_train\"\n",
        "val_history = model_folder + name_model + \"_\" + \"history_val\"\n",
        "\n",
        "\n",
        "# Learning Rate schedule: decays the learning rate by a factor of `gamma` .\n",
        "# every `step_size` epochs.\n",
        "scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4R6KugZ2jUzs"
      },
      "outputs": [],
      "source": [
        "mkdir_model(base_dir, name_model, 0)\n",
        "# Train and evaluate\n",
        "model, train_hist, val_hist, best_acc = train_model(model, dataloaders_dict, criterion, optimizer_ft,scheduler, num_epochs=num_epochs, \n",
        "                                          is_inception=False)\n",
        "#Saving the updated model for the inference phase\n",
        "torch.save(model.state_dict(), model_file)\n",
        "\n",
        "# Save histories data\n",
        "save_history(train_hist, train_history)\n",
        "save_history(val_hist, val_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcV_0RB6jYJh"
      },
      "outputs": [],
      "source": [
        "# Num epochs for this snippet\n",
        "num_epochs = 10\n",
        "\n",
        "# model general info\n",
        "name_model = \"vfer_small_15\"\n",
        "base_dir = \"../Models/\"\n",
        "mkdir_model(base_dir, name_model, 0)\n",
        "\n",
        "# model files for saving history and model data\n",
        "model_folder = base_dir + name_model + \"/\"\n",
        "model_file = model_folder + name_model + \".pth\"\n",
        "train_history = model_folder + name_model + \"_\" + \"history_train\"\n",
        "val_history = model_folder + name_model + \"_\" + \"history_val\"\n",
        "\n",
        "# changing starting lr\n",
        "lr_in = 0.01\n",
        "optimizer_ft = optim.Adam(model.parameters(), lr=lr_in, betas=(momentum_in, 0.9))\n",
        "scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
        "\n",
        "# Train and evaluate\n",
        "model, train_hist, val_hist, best_acc = train_model(model, dataloaders_dict, criterion, optimizer_ft,scheduler, num_epochs=num_epochs, \n",
        "                                          is_inception=False, is_loaded=True, model_folder= model_folder, best_acc=best_acc,\n",
        "                                          load_state_ws=\"../Models/vfer_small_5/vfer_small_5.pth\")\n",
        "\n",
        "\n",
        "#Saving the updated model for the inference phase\n",
        "torch.save(model.state_dict(), model_file)\n",
        "\n",
        "# Save histories data\n",
        "save_history(train_hist, train_history)\n",
        "save_history(val_hist, val_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wgtdw0D8jeCK"
      },
      "outputs": [],
      "source": [
        "# model general info\n",
        "name_model = \"vfer_sam_25\"\n",
        "base_dir = \"/content/drive/MyDrive/Models/\"\n",
        "mkdir_model(base_dir, name_model, 0)\n",
        "\n",
        "# model files for saving history and model data\n",
        "model_folder = base_dir + name_model + \"/\"\n",
        "model_file = model_folder + name_model + \".pth\"\n",
        "train_history = model_folder + name_model + \"_\" + \"history_train\"\n",
        "val_history = model_folder + name_model + \"_\" + \"history_val\"\n",
        "\n",
        "# updating num_epochs\n",
        "num_epochs = 5\n",
        "# changing starting lr\n",
        "lr_in = 0.001\n",
        "optimizer_ft = optim.SGD(model.parameters(), lr=lr_in, momentum=momentum_in)\n",
        "scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
        "\n",
        "# Train and evaluate\n",
        "model, train_hist, val_hist, best_acc = train_model(model, dataloaders_dict, criterion, optimizer_ft,scheduler, num_epochs=num_epochs, \n",
        "                                          is_inception=False, is_loaded=True, model_folder= model_folder,\n",
        "                                          load_state_ws=\"/content/drive/MyDrive/Models/vfer_sam_10/vfer_sam_10.pth\", best_acc=best_acc )\n",
        "\n",
        "\n",
        "#Saving the updated model for the inference phase\n",
        "torch.save(model.state_dict(), model_file)\n",
        "\n",
        "# Save histories data\n",
        "save_history(train_hist, train_history)\n",
        "save_history(val_hist, val_history)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMqs6b1onWpO+pSVpaSuSM8",
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "Training.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
